{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment-based Music Generation for Video Games\n",
        "### A Computational Creativity Project by Tyler H. McIntosh\n",
        "\n",
        "The sections below serve as a guide to using our system. We have provided our trained models so that you don't have to spend 30+ hours training them yourself. Even so, we have included the training functions just in case you are curious - though, we have commented them out. If you are unsure, avoid any sections containing the <font color='red'>[Training only]</font> tag. All enquiries should be directed to Hello@TylerHMc.com."
      ],
      "metadata": {
        "id": "maYP4EXSM1LB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjMEOjl3QMh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!pip install pypianoroll\n",
        "clear_output()\n",
        "from keras.utils.data_utils import get_file\n",
        "from IPython.display import HTML, display\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.backend import sigmoid\n",
        "from keras.utils import np_utils\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pypianoroll as p\n",
        "from PIL import Image\n",
        "from music21 import *\n",
        "import numpy as np\n",
        "import shutil\n",
        "import glob\n",
        "import os\n",
        "\n",
        "!mkdir output\n",
        "try:\n",
        "  shutil.rmtree('/content/sample_data')\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<br />"
      ],
      "metadata": {
        "id": "bG6egwcEUdmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seed Generation"
      ],
      "metadata": {
        "id": "yjYNU2W9QVTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we must load our two datasets: Img2Mid and ZeldaMIDI. These datasets were created specifically for this project; Img2Mid being a set of Zelda screenshots and MIDI pairs, and ZeldaMIDI being a much larger collection of Zelda music. The datasets are stored on a personal server, and are downloaded/unzipped by the function bellow."
      ],
      "metadata": {
        "id": "P6g5PbBhOE91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the Datasets\n",
        "\n",
        "!wget https://www.tylerhmc.com/datasets/Img2Mid.zip\n",
        "!wget https://www.tylerhmc.com/datasets/ZeldaMIDI.zip\n",
        "!wget https://www.tylerhmc.com/datasets/sample_image.jpg\n",
        "!unzip \"Img2Mid.zip\" -d \"/content/Img2Mid\"\n",
        "!unzip \"ZeldaMIDI.zip\" -d \"/content/ZeldaMIDI\"\n",
        "!rm Img2Mid.zip\n",
        "!rm ZeldaMIDI.zip\n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "U4-qdOhYLL35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the variational autoencoder (VAE) that serves as the seed generator. We use convolution layers to allow the VAE to interpret multidimensional features. This network takes the screenshots from the Img2Mid dataset as inputs, and the corresponding MIDI tracks as outputs. While training, it learns to associate the pictures of the game with the music found in that area. The trained network is able to produce a short MIDI seed from any image based on the similarity of its features to the training data."
      ],
      "metadata": {
        "id": "ktd0M1BSPeCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the VAE\n",
        "\n",
        "# Based on Keras example implementation\n",
        "# https://keras.io/examples/generative/vae/\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            x, y = data\n",
        "            z_mean, z_log_var, z = self.encoder(x)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(y, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# Modified input shape and structure\n",
        "encoder_inputs = keras.Input(shape=(306, 544, 3))\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(10, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(10, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "latent_inputs = keras.Input(shape=(10,))\n",
        "x = layers.Dense(16*65*64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((16, 65, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eZumz_XoDu59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this network does not take terribly long to train, it learns stochastically - meaning its output will not always be exactly the same every time it is trained. Therefore, we do not recommend trying to train it yourself, as it can be something of a rabbit hole. Future work will look into making this process deterministic - for example, by using a deterministic regularized autoencoder. For now, use the \"Load the trained VAE\" function."
      ],
      "metadata": {
        "id": "nEaR5xrMQnxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFNJjxfAEcOc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train the VAE <font color='red'>[Training only]</font>\n",
        "\n",
        "# def findStart(array):               # Function to shift piano roll to first note\n",
        "#   for i in range(array.shape[1]):   # For every (x,y) in piano roll\n",
        "#     for j in range(array.shape[0]): #\n",
        "#       if array[j,i] == True:        # If there is a note\n",
        "#         return array[:,i:]          # Slice piano roll to position of first note\n",
        "\n",
        "# def loadData(midiPath, imagePath):  # Loads the Img2Mid dataset\n",
        "#   midi = list()                     # Initialise midi list\n",
        "#   image = list()                         # Initialise image list\n",
        "#   for filename in os.listdir(midiPath):  # For every midi in the midi path\n",
        "#     imageName = filename[:-4]+'.png'     # Set image path\n",
        "#     if '.mid' in filename:                    # If is a midi file\n",
        "#       if imageName in os.listdir(imagePath):  # If there is an image for it\n",
        "#         s = p.read(midiPath+filename)         # Load the midi file\n",
        "#         try:                                  # Try to merge left and right hand\n",
        "#           s = p.binarize(s).stack()[0] + p.binarize(s).stack()[1] # By combinging them\n",
        "#         except:                                                   # If only 1 channel\n",
        "#           s = p.binarize(s).stack()[0]                      # Just use that one channel\n",
        "#         s = np.flip(np.swapaxes(s,0,1),0)             # Flip time and notes\n",
        "#         s = findStart(s)[:,:520]              # Slice to the start\n",
        "#         s = np.expand_dims(s, axis=-1)*1      # Add feature dimension\n",
        "#         midi.append(s)                        # Add to midi list\n",
        "#         print(filename)                       # Log progress\n",
        "#         i = Image.open(imagePath+imageName)   # Load the image\n",
        "#         i.thumbnail((544,306))                # Downsample\n",
        "#         i = i.resize((544,306))               # Cut to size\n",
        "#         i = np.array(i)                       # Convert to np array\n",
        "#         image.append(i)                       # Add to images\n",
        "#   return np.array(image), np.array(midi)      # Return dataset\n",
        "\n",
        "# x_train, y_train = loadData(midiPath='/content/Img2Mid/', imagePath='/content/Img2Mid/')\n",
        "# x_train, y_train = x_train/255, y_train  # Normalise\n",
        "\n",
        "# vae = VAE(encoder, decoder)                    # Initialise VAE\n",
        "# vae.compile(optimizer=keras.optimizers.Adam()) # Compile\n",
        "# vae.fit(x_train, y_train, epochs=2500)         # Train for 2.5k epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the trained VAE\n",
        "\n",
        "vae = VAE(encoder, decoder)                     # Initialize\n",
        "vae.compile(optimizer=keras.optimizers.Adam())  # Compile\n",
        "vae.built = True                                # Build\n",
        "vae.load_weights(get_file('VAE', 'https://www.tylerhmc.com/models/seedGenerator.hdf5')) # Load from server"
      ],
      "metadata": {
        "id": "o34BeONo-3S4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, a helper function that allows us to grab an image and generate a seed using the trained VAE. In essence, this function loads the image, transforms it into a standardised thumbnail to fit the dimensions of the model, passes it through the VAE, applies a threshold to the output, and then saves the output into a MIDI file called \"seed.mid\"."
      ],
      "metadata": {
        "id": "LPGKkKcER6rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Seed generator helper function\n",
        "\n",
        "def generateSeed(path):                # Generate VAE prediction\n",
        "  try:                                 # Try to open file\n",
        "    i = Image.open(path).convert('RGB') # Open input image\n",
        "  except:                              # If not found\n",
        "    print('File not found')            # Raise error\n",
        "    return                             # Exit\n",
        "  i.thumbnail((544,306))               # Downsample\n",
        "  i = i.resize((544,306))              # Cut to size\n",
        "  i = np.expand_dims(i,axis=0)         # Add batch dim\n",
        "  i = i / 255                          # Normalize\n",
        "  i = vae.encoder.predict(i)           # Encode to latent\n",
        "  i = vae.decoder.predict(i[0])        # Generate prediction\n",
        "  i = np.squeeze(i[0], axis=-1)        # Remove feature dim\n",
        "  i = np.where(i<0.8, 0, i)            # Threshold\n",
        "  i = np.where(i>0.8, 1, i)            # Binarize\n",
        "  if np.any(i) == False:                # If seed is empty\n",
        "    print('Sorry, please try another image') # Raise error\n",
        "    return                             # Exit\n",
        "  plt.imshow(i, interpolation='none')  # Generate piano roll\n",
        "  plt.show()                           # Display piano roll\n",
        "  i = np.flip(i,0).astype(int)         # Reverse note order\n",
        "  i = np.swapaxes(i,0,1)               # Swap time and notes\n",
        "  i = [p.BinaryTrack(name='Staff-1', program=1, is_drum=False, pianoroll=i)] # Create midi track\n",
        "  i = p.Multitrack(tracks=i)  # Create multitrack object\n",
        "  return i                    # Return multitrack"
      ],
      "metadata": {
        "id": "ynb_crU7FkUD",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to generate a MIDI seed. Find an image that you would like to turn into music and upload it to your Colab session. The closer its aspect ratio is to 16:9 the better, but this is not a requirement for it to work. Type the name of your image into the field below, and make sure to include the filetype (e.g., \"cat.png\"). Alternatively, you can use the sample image we have provided. Then, click the run button. The system will pass your image through the VAE and write a MIDI seed file to the output folder. If it doesn't pop up in your files immediately, give it a few seconds - Colab can be kinda slow."
      ],
      "metadata": {
        "id": "u_mvbqbDSwr6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39zsVhZmJ6vA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate a Seed <font color='limegreen'>+</font>\n",
        "Filename = 'sample_image.jpg' #@param {type:\"string\"}\n",
        "try:\n",
        "  p.write(\"output/seed.mid\", generateSeed(Filename)) # Generate seed\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<br />"
      ],
      "metadata": {
        "id": "feYk89kgUgf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motif Generation"
      ],
      "metadata": {
        "id": "5iMAFtGXTshv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a seed file to generate the music with, we can move onto to the next section of the system; the motif generator. This section uses a Long Short-Term Memory (LSTM) network to generate next-note predictions in a loop. If we give it the seed as the input, it will generate a composition based on it. First, we need to define the LSTM network and a few helper functions. This may take a minute or two, as the system needs to build a dictionary of notes from the dataset."
      ],
      "metadata": {
        "id": "ILZKc_iATxi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the LSTM\n",
        "# LSTM network and helper functions\n",
        "\n",
        "def progress(value, max=100):         # Progress bar function\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max)) # Some basic html\n",
        "\n",
        "def midiGenerator(network_input, n_vocab):  # LSTM network\n",
        "    model = Sequential([ # Standard LSTM, lots of dropout\n",
        "      layers.LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), recurrent_dropout=0.2, return_sequences=True),\n",
        "      layers.LSTM(512, return_sequences=True, recurrent_dropout=0.2),\n",
        "      layers.LSTM(512),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(n_vocab, activation='relu'),\n",
        "      layers.BatchNormalization(),\n",
        "      layers.Dropout(0.2),\n",
        "      layers.Dense(n_vocab, activation='softmax'),\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "    return model\n",
        "\n",
        "def get_notes(path, define=False):      # Retrieve note list\n",
        "    notes = list()                      # Initialize notes\n",
        "    if define:                          # If define\n",
        "      out = display(progress(0, 162), display_id=True) # Initialise progress bar\n",
        "      x=0                                # Initialise progress counter\n",
        "    for file in glob.glob(path):         # For MIDI in path\n",
        "        midi = converter.parse(file)     # Load MIDI\n",
        "        notes_to_parse = midi.flat.notes   # Convert to list of note objects\n",
        "        for element in notes_to_parse:           # For note in list\n",
        "            if isinstance(element, note.Note):     # If is single note\n",
        "                notes.append(str(element.pitch))     # Add pitch to notes\n",
        "            elif isinstance(element, chord.Chord):   # If chord\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder)) # Encode and add chord code to list\n",
        "        if define:                       # If define\n",
        "          x+=1                           # Iterate progress\n",
        "          out.update(progress(x, 162))   # Update progress\n",
        "    return notes  # Return list of notes\n",
        "\n",
        "def prepare_train_sequences(notes, n_vocab):  # Create time-stepped training sets\n",
        "    sequence_length = 50                              # 50 note long chunks\n",
        "    pitchnames = sorted(set(item for item in notes))  # Grab list of unique pitches\n",
        "    # Generate note to index dictionary map\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    network_input = []   # Initialise train input\n",
        "    network_output = []  # Initialise train output\n",
        "\n",
        "    for i in range(0, len(notes) - sequence_length, 1):  # For index in note window\n",
        "        sequence_in = notes[i:i + sequence_length]       # Grab notes in sliding window\n",
        "        sequence_out = notes[i + sequence_length]        # Grab note after that\n",
        "        network_input.append([note_to_int[char] for char in sequence_in]) # Convert to index values and add to train input\n",
        "        network_output.append(note_to_int[sequence_out])                  # Convert to index value and add to train output\n",
        "\n",
        "    n_patterns = len(network_input)  # Grab number of input samples\n",
        "\n",
        "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1)) # Reshape input to fit network\n",
        "    network_input = network_input / float(n_vocab)                              # Normalize\n",
        "\n",
        "    network_output = np_utils.to_categorical(network_output)  # Convert to class vector\n",
        "\n",
        "    return network_input, network_output, pitchnames, note_to_int  # Return input, output, unique pitches, and mapping dictionary\n",
        "\n",
        "notes = get_notes('ZeldaMIDI/*.mid', define=True)  # Grabs notes in train set\n",
        "n_vocab = len(set(notes))                          # Calc number of unqiue values\n",
        "network_input, network_output, pitchnames, direct = prepare_train_sequences(notes, n_vocab)  # Generate training data and dict map\n",
        "midiGen = midiGenerator(network_input, n_vocab)                         # Initialise LSTM\n",
        "midiGen.compile(loss='categorical_crossentropy', optimizer='RMSprop')   # Compile\n",
        "clear_output()                                                          # Clear output"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iacpkPZgAgnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the VAE, the LSTM network takes an **extremely** long time to train. In our case, we spent roughly 30 hours training it on a Tesla P100 GPU, and it still is not perfect. However, the training time could be greatly reduced using a TPU. So, if you're feeling up to it, you can try to improve the model yourself... Or you can save yourself the hastle and use the trained model we supply below."
      ],
      "metadata": {
        "id": "QG3rsCyPVGzZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUI0SEM19BUv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train the LSTM <font color='red'>[Training only]</font>\n",
        "# midiGen.fit(network_input, network_output, epochs=200)  # Train network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the pre-trained LSTM\n",
        "\n",
        "# Load weights from server\n",
        "midiGen.load_weights(get_file('LSTM', 'https://www.tylerhmc.com/models/motifGenerator.hdf5'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dKp_DCb6BQBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for some motif generation helper functions. These functions allow us to load the seed file, turn it into a list of notes, pass it through the LSTM, and turn the resulting list of generated notes into a MIDI file called \"motif.mid\"."
      ],
      "metadata": {
        "id": "u0y0yvroXMEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH-lyMUUCONb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Motif Generator helper functions\n",
        "\n",
        "def generate(n_vocab, pitchnames, notes, direct):  # Generate motif function\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab, direct) # Process seed\n",
        "    prediction_output = generate_notes(midiGen, network_input, pitchnames, n_vocab, direct) # Generate prediction\n",
        "    output = create_midi(prediction_output)  # Create MIDI\n",
        "    return output                            # Return MIDI\n",
        "\n",
        "def prepare_sequences(notes, pitchnames, n_vocab, direct): # Similar to training version\n",
        "\n",
        "    note_to_int = direct  # Load the previously created mapping dictionary\n",
        " \n",
        "    sequence_length = 50   # Set sequence length\n",
        "    network_input = list() # Initialise input\n",
        "\n",
        "    for note in notes:               # For note in the seed\n",
        "      if note not in direct.keys():  # If note not in dictionary\n",
        "        notes.remove(note)           # Remove it\n",
        "        # This occurs when two notes overlap and generate\n",
        "        # a chord that hasn't been seen previously\n",
        "\n",
        "    while len(notes) < sequence_length: # If seed is short than min length\n",
        "      notes += notes                    # Double it\n",
        "\n",
        "    sequence_in = notes[0:sequence_length]  # Slice first 50 notes\n",
        "    network_input.append([note_to_int[char] for char in sequence_in]) # Convert to index\n",
        "\n",
        "    normalized_input = np.reshape(network_input, (1, sequence_length, 1)) # Reshape to fit network\n",
        "    normalized_input = normalized_input / float(n_vocab) # Normalize\n",
        "\n",
        "\n",
        "    return (network_input, normalized_input)  # Return input, normalised input\n",
        "\n",
        "def generate_notes(model, network_input, pitchnames, n_vocab, direct):  # Prediction loop\n",
        "\n",
        "    int_to_note = dict((v,k) for k,v in direct.items())  # Swap keys with values in mapping dictionary\n",
        "\n",
        "    pattern = network_input[0]  # Grab length\n",
        "    prediction_output = list()  # Initialize predictions\n",
        "\n",
        "    # generate 100 notes\n",
        "    for note_index in range(100):  # For 100 cycles\n",
        "        prediction_input = np.reshape(pattern, (1, len(pattern), 1)) # Reshape to fit network\n",
        "        prediction_input = prediction_input / float(n_vocab)         # Normalise\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)  # Predict next note\n",
        "\n",
        "        index = np.argmax(prediction)  # Return index of highest output\n",
        "        result = int_to_note[index]       # Convert to note names\n",
        "        prediction_output.append(result)  # Add pred name to pred list\n",
        "\n",
        "        pattern.append(index)             # Add pred index to input\n",
        "        pattern = pattern[1:len(pattern)] # Remove first entry\n",
        "\n",
        "    return prediction_output  # Return prediction\n",
        "\n",
        "def create_midi(prediction_output):  # Creates a midi object from prediction\n",
        "    offset = 0                             # Initialise offset\n",
        "    output_notes = []                      # Initialise output\n",
        "    for pattern in prediction_output:              # For entry in preds\n",
        "        if ('.' in pattern) or pattern.isdigit():  # If entry is chord\n",
        "            notes_in_chord = pattern.split('.')    # Retrieve notes in chord\n",
        "            notes = []                             # Reset notes\n",
        "            for current_note in notes_in_chord:         # For note in chord\n",
        "                new_note = note.Note(int(current_note)) # Create note object\n",
        "                new_note.storedInstrument = instrument.Piano() # Set program\n",
        "                notes.append(new_note)             # Add note object to chord\n",
        "            new_chord = chord.Chord(notes)         # Create chord object\n",
        "            new_chord.offset = offset              # Add offset\n",
        "            output_notes.append(new_chord)         # Add chord to output\n",
        "        else:                                      # If is a single note\n",
        "            new_note = note.Note(pattern)          # Create note object\n",
        "            new_note.offset = offset               # Add offset\n",
        "            new_note.storedInstrument = instrument.Piano() # Set program\n",
        "            output_notes.append(new_note)          # Add note to output\n",
        "\n",
        "        offset += 0.5  # Increment offset to stop notes overlapping\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes) # Create MIDI object from output\n",
        "\n",
        "    midi_stream.write('midi', fp='output/motif.mid') # Save as MIDI file\n",
        "    return output_notes                              # Return notes object"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can use the section below to load the seed file, using the helper functions discussed above, and turn it into a generated motif. This function also returns the \"output\" variable, which is used later to generate an expressive performance."
      ],
      "metadata": {
        "id": "Vr7oCDLqX1DI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io-O2Ts-Eu6n",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate motif <font color='limegreen'>+</font>\n",
        "\n",
        "seed = get_notes('output/seed.mid')  # Load seed\n",
        "output = generate(n_vocab, pitchnames, seed, direct) # Generate motif"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<br>"
      ],
      "metadata": {
        "id": "3PW-f6ffYKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expressive Performance Rendering"
      ],
      "metadata": {
        "id": "jC9uXQLVYNil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we now have a musical motif generated from an image, but why stop there? The LSTM network only produces a list of notes in the order by which they are played. This isn't terribly musical, and certainly wouldn't win any game music awards. Therefore, we need to make use of the Expressive Performance Rendering (EPR) models detailed below. These are two fairly straight forward convolutional regression networks performing sequence-to-sequence vector translation. They take a list of 100 notes as input and output both the time-offsets and velocities that the notes *should* have. These separate networks are both trained using the same ZeldaMIDI dataset used to train the LSTM, but instead of the targets being the same notes time-shifted, they are either relative offsets or velocities. First, we must define them using the function below."
      ],
      "metadata": {
        "id": "WCQzcMGIYtZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su4kPKBrdMTu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define the EPR models\n",
        "\n",
        "def durations():  # Durations model architecture\n",
        "    model = Sequential([\n",
        "      layers.Input(shape=(100, 1)),\n",
        "      layers.Conv1D(filters=8, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Dense(300, activation='sigmoid'), # Sigmoid for regression\n",
        "      layers.Dense(200, activation='sigmoid'),\n",
        "      layers.Dense(100, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
        "    return model # MSE for regression\n",
        "\n",
        "def velocity(): # Velocity model architecture\n",
        "    model = Sequential([\n",
        "      layers.Input(shape=(100, 1)),\n",
        "      layers.Conv1D(filters=8, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Conv1D(filters=16, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "      layers.Dense(300, activation='sigmoid'),\n",
        "      layers.Dense(200, activation='sigmoid'),\n",
        "      layers.Dense(100, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
        "    return model\n",
        "\n",
        "durro = durations() # Initialise durration model\n",
        "velo = velocity()   # Initialise velocity model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, you have the option of training these models yourself using the training function below. However, this is not recommended, as they can take quite a while to train depending on your hardware, and there is nothing to gain by doing so. Instead, you can use the trained models we have supplied by running the \"Load trained EPR models\" function below."
      ],
      "metadata": {
        "id": "--1up12raySz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DVQaGWchTsD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create EPR dataset and train <font color='red'>[Training only]</font>\n",
        "\n",
        "# Creates both the duration and velocity datasets at the same time\n",
        "\n",
        "# train_x = list()         # Initalize input data\n",
        "# durro_train_y = list()   # Initalize durration output\n",
        "# velo_train_y = list()    # Initalize velocity output\n",
        "# for file in glob.glob('ZeldaMIDI/*.mid'): # For MIDI in database\n",
        "#   midi = converter.parse(file)            # Load MIDI\n",
        "#   print(\"Parsing %s\" % file)              # Log progress\n",
        "#   notes_to_parse = midi.flat.notes        # Convert to list of note objects\n",
        "#   step = 0                                # Reset step\n",
        "#   while step+100 <= len(notes_to_parse):  # While in range\n",
        "#     notes = list()                        # Reset notes\n",
        "#     offsets = list()                      # Reset offsets\n",
        "#     velocity = list()                     # Reset velocity\n",
        "#     rel_offsets = list()                  # Reset relative offsets\n",
        "#     for element in notes_to_parse[step:step+100]:  # For entry in notes\n",
        "#       if isinstance(element, note.Note):           # If a single note\n",
        "#         notes.append(str(element.pitch.midi))      # Add to notes\n",
        "#       elif isinstance(element, chord.Chord):       # If a chord\n",
        "#         notes.append(''.join(str(n) for n in element.normalOrder)) # Encode notes\n",
        "#       offsets.append(float(element.offset))         # Add offset to offsets\n",
        "#       velocity.append(str(element.volume.velocity)) # Add velocity to velocities\n",
        "#     train_x.append(notes)                           # Add notes to train set\n",
        "#     for i in range(len(offsets)-1):                                # For every entry in offsets\n",
        "#       rel_offsets.append(abs(round(offsets[i+1] - offsets[i],3)))  # Calulate distance between offsets\n",
        "#     rel_offsets.append(rel_offsets[-1])                            # Duplicate last (has no effect, prevents error)\n",
        "#     durro_train_y.append(rel_offsets)               # Add relative offsets to duration train set\n",
        "#     velo_train_y.append(velocity)                   # Add velocties to velocities train set\n",
        "#     step+=1                                         # Increment step\n",
        "\n",
        "# train_x = np.expand_dims(np.array(train_x), -1)             # Add feature dim\n",
        "# durro_train_y = np.expand_dims(np.array(durro_train_y), -1) # Add feature dim\n",
        "# velo_train_y = np.expand_dims(np.array(velo_train_y), -1)   # Add feature dim\n",
        "\n",
        "# train_y = np.clip(durro_train_y,0.,2.)/2  # Limit durations to 2 sec and normalize\n",
        "\n",
        "# train_x = train_x.astype('float32')             # Convert to float\n",
        "# durro_train_y = durro_train_y.astype('float32') # Convert to float\n",
        "# velo_train_y = velo_train_y.astype('float32')   # Convert to float\n",
        "\n",
        "\n",
        "# durro.fit(train_x, durro_train_y, epochs=2000)  # Train durration model\n",
        "# velo.fit(train_x, velo_train_y, epochs=2000)    # Train velocities model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load trained EPR models\n",
        "\n",
        "# Load weights from server\n",
        "velo.load_weights(get_file('VELO', 'https://www.tylerhmc.com/models/velocity.hdf5'))\n",
        "durro.load_weights(get_file('DURO', 'https://www.tylerhmc.com/models/durations.hdf5'))"
      ],
      "metadata": {
        "id": "yZJP-cx4HcH9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can pass our motif through the EPR models to render the final expressive performance. This produces a file called \"performance.mid\" containing the final rendering of the composition. As it is supplied in MIDI format, you can apply any instrument voice to it you like, and can modify it as much as you want. We suggest that you use it as inspiration for a new original composition, carrying on the cycle of creativity, or you can simply return to the start and try again with a new image.\n",
        "\n",
        "*Note: You only need to re-run the cells with \"<font color='limegreen'>+</font>\" in the title.*"
      ],
      "metadata": {
        "id": "RB-azlc0eV7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate expressive performance <font color='limegreen'>+</font>\n",
        "\n",
        "output_notes = list()     # Initialise output\n",
        "for element in output:    # For note in the motif output\n",
        "  if isinstance(element, note.Note):  # If is a single note\n",
        "    output_notes.append(float(element.pitch.midi))  # Add the MIDI pitch value to output\n",
        "  elif isinstance(element, chord.Chord):            # If is a chord\n",
        "    output_notes.append(float(''.join(str(n) for n in element.normalOrder))) # Add encoded value to output\n",
        "\n",
        "output_notes = np.expand_dims(np.expand_dims(np.array(output_notes), -1), 0) # Add feature and batch dims\n",
        "offsets = np.round(durro.predict(output_notes), 2)[0].tolist()  # Generate durations\n",
        "velocity = velo.predict(output_notes)[0].tolist()               # Generate velocities\n",
        "\n",
        "offset = 0                                           # Reset offset\n",
        "for i in range(len(output)):                      # For every note in output\n",
        "  offset += (0.25 * round((offsets[i][0]*2)/0.25))   # Increment offset by pred rounded to quater note\n",
        "  output[i].offset = offset                          # Add quantized offset to note\n",
        "  output[i].volume.velocity = velocity[i][0]*127     # Add velocity to note\n",
        "\n",
        "midi_stream = stream.Stream()                           # Initialize MIDI object\n",
        "midi_stream.append(tempo.MetronomeMark(number=100))     # Set standard tempo\n",
        "midi_stream.append(stream.Stream(output))               # Add output to MIDI steam\n",
        "midi_stream.write('midi', fp='output/performance.mid')  # Write MIDI to file"
      ],
      "metadata": {
        "id": "BSadNP0MSyIr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "PVAuDldjfyCf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Img2Mid.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}